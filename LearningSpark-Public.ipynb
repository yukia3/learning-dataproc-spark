{"nbformat_minor":2,"cells":[{"execution_count":null,"cell_type":"code","source":["# -------------- Apache Sparkについて -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","Apache Sparkは、簡単に言うと 並列分散処理基盤である Hadoop の MapReduce フレームワークに相当するもの。\n","\n","MapReduce処理は Map処理→Reduce処理 ごとに基本的には HDFS に書き込んでいる。\n","その場合、\n","　・Map処理→Reduce処理が多段になった場合に、I/Oのレイテンシが問題になる\n","　・機械学習のような、同じデータ（処理結果）を何度も使い回す処理の場合、やはりI/Oが何度も発生し問題になる\n","といったような問題が発生する。\n","\n","そこで Apache Spark では “インメモリ”・“RDD”（部分故障への耐性を考慮した分散コレクション） を活用することで\n","この問題を解決していく。\n","\n","このRDDは Apache Spark Core として低レイヤを担っており、\n","Apache Spark Core をベースとして\n","　・Spark SQL\n"," 　（構造化されたデータの処理のためのSparkコンポーネント、構造化されていれば形式に関わらず、DataFrameとして読み込むことでSQL的にデータを扱える）\n"," ・Spark Streaming\n"," 　（ストリームデータを処理するためのSparkコンポーネント）\n"," ・MLib\n"," 　（機械学習のためのSparkコンポーネント、最新ではSpark SQLに入っている？）\n"," ・GraphX\n"," 　（大容量のグラフデータを並列分散環境で処理するためのコンポーネント、ナレッジグラフ、マーケティングリレーション、経路探索、ページランク分析など特定の分野に対しては非常に有効）\n","といったコンポーネントが用意されている。\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# -------------- 前提としてやるべきこと -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","SparkSessionをimportする\n","\"\"\"\n","from pyspark.sql import SparkSession"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","SparkSessionでセッションを作成する。\n","なお、すでに同一名のセッションが存在する場合は既存セッションを取得する。\n","\"\"\"\n","spark = SparkSession \\\n","  .builder \\\n","  .master('yarn') \\\n","  .appName('learning-spark') \\\n","  .getOrCreate()"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","Dataprocではデータエクスポートの一時ディレクトリとして、GCSを利用することができる。\n","特にBigQueryコネクタを利用して取得する際には重要になる\n","\"\"\"\n","bucket = \"〜\"\n","spark.conf.set('temporaryGcsBucket', bucket)"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# -------------- Spark SQL -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# -------------- DataFrame と RDD -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","pyspark.sql で利用できるデータ構造としては、大きく DataFrame と RDD がある。\n","RDD（Resilient Distributed Dataset）は Spark Core で提供されており、Sparkの中心となるデータ構造であり、これまでSpark SQLではRDDがメインで使われていた。\n","\n","しかしその後、pyspark.sqlではより上位のデータ抽象化として DataFrame が登場した。RDDは「Sparkが目的をどう実現しているか」を表すのに対して、DataFrameは「Sparkは意味的に何をしているのか」が表現しやすい。\n","\n","DataFrameによって、様々な形式のデータ（CSVなどのファイル、RDBやBigQueryなど）を、まるでRDBに対してSQLで操作するように扱うことができると共に、言語間（ScalaとPython、Rなど）での実装の差を大きく減らすことができた。\n","\n","RDDに対してDataFrameは、位置付けとしてより高レイヤなオブジェクトであり、filterやjoinなどの便利なメソッドが用意されている。\n","用意されているAPIでできる処理は、DataFrameの方がパフォーマンスも出せるため、今後は原則としてDataFrameで処理を組んでいくことが基本指針となる。\n","\n","また、RDDへの変換・RDDからの変換も可能であり、以下のような要件が出てきた場合には、RDDに変換して対応していく、ということになる。\n","　・RDDで構築された3rd Partyのパッケージを利用する場合\n","　・Sparkにクエリの実行方法を正確に指示したい場合\n","\n","※   また Spark としては 「Datasets」 という型定義できるデータ構造もあるが、\n","    コンパイルを伴う Java か Scala でしか利用できない。\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","DataFrameは、sparkセッションを使って\n","　・データファイルをreadする\n","　・他のDataFrameをフィルタリングする\n","などした場合に生成できる。（SparkSessionが安全に、分散処理を実現してくれる）\n","\n","DataprocではデフォルトでGCSコネクタが入っており、\n","pathに “gs://〜” と対象ファイルを指定することで、自然に取得することができる。\n","\"\"\"\n","tsv_path=\"gs://〜.tsv\"\n","df = spark.read.csv(tsv_path, sep=r'\\t', header=True)"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","limit や selectで、SQLライクにデータを扱える\n","\"\"\"\n","df.limit(5).select(\"text\").show()\n","print(df.count())\n","print(df.columns)"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","RDDへの変換は df.rdd で行える。\n","ただし RDD[Row] 形式で返ってくるので、map内ではさらにフィールドを指定してやる必要あり （selectで特定フィールドに絞ってもダメ）\n","https://stackoverflow.com/questions/40653567/attribute-error-split-on-spark-sql-python-using-lambda\n","\"\"\"\n","df.limit(4).select('idx_id').rdd.map(lambda line: line.idx_id.split(\"_\")).collect()"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","RDDからDataFrameの変換も行える\n","https://blog.imind.jp/entry/2019/06/23/004922\n","\"\"\"\n","from pyspark.sql import types as T, functions as F\n","\n","# スキーマの設定\n","schema = T.StructType([\n","    T.StructField('col1', T.StringType()),\n","    T.StructField('col2', T.LongType())\n","])\n","\n","# 先ほどエラーになったrdd\n","rdd2 = sc.parallelize([\n","    Row(col1=None, col2=1),\n","    Row(col1=None, col2=2)\n","])\n","\n","# schemaを指定してDataFrameに変換\n","df2 = spark.createDataFrame(rdd2, schema)\n","df2.collect()"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","Spark SQLをjupyterで触っていて、selectなどを使っていると「あれ？こんなに早くselectできるの？」と思う場面があるかもしれない。\n","\n","limitやselectなどのfuncを呼んでいる際には処理の流れを組んでいる状態\n","（これらの処理は「Transformations」と呼ばれる場合も。\n","　RDDで言うと、https://ex-ture.com/blog/2019/06/27/learn-databricks-spark-rdd-operations/\n","    map\n","    flatMap\n","    filter\n","    union\n","    intersection\n","    subtrct\n","    distinct\n","　　などがあり、DataFrameで言うと https://www.learningjournal.guru/courses/spark/spark-foundation-training/spark-dataframe-transformations/\n","     select\n","    groupby\n","　などがある）\n","\n","それが実際に処理されるのは「Actions」という“結果を取得するfunc”が呼ばれたときに初めて実行される。\n","RDDで言うと、https://ex-ture.com/blog/2019/06/27/learn-databricks-spark-rdd-operations/\n","    collect\n","    count\n","    first\n","    take\n","    reduce\n","    takeOrdered\n","    top\n","などがあり、DataFrameで言うと \n","    show\n","や、各種出力が挙げられる。\n","\n","Actionsの多くは、結果としてPythonの配列を返すことも多い。\n","（逆を言うと、RDDはSparkの型なので、通常のPythonのfunc処理を使おうとする際には注意が必要）\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","DataFrameのUDF\n","RDDではmapやfilterで各レコードごとに処理をしていくが、\n","DataFrameではUDF（ユーザー定義関数）を使って処理をしていく。\n","https://spark.apache.org/docs/2.4.0/api/python/_modules/pyspark/sql/udf.html\n","\"\"\"\n","class JapaneseTokenizer(object):\n","    def __init__(self):\n","        self.mecab = MeCab.Tagger(\"-Ochasen -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n","        self.mecab.parseToNode('')\n"," \n","    def split(self, text):\n","        node = self.mecab.parseToNode(text)\n","        words = []\n","        while node:\n","            if node.surface:\n","                words.append(node.surface.decode(\"UTF-8\"))\n","            node = node.next\n","        return words\n","def tokenize(text):\n","    tokenizer = JapaneseTokenizer()\n","    return tokenizer.split(text)\n","def tokenize_and_create_rdd(text):\n","    return ','.join(tokenize(text.encode(\"UTF-8\")))\n","\n","tokenize_udf = F.udf(tokenize_and_create_rdd, T.StringType()) #既存のdefを第一引数に、第二引数に戻り値のSpark型を入れて教えてあげる\n","\n","df_wakati_result = df_wakati_base\\\n","    .withColumn(\"wakati\", tokenize_udf((F.col(\"text\"))))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","UDFについて\n","PySparkのUDFはかつて、Scala UDFよりも遅いという課題を抱えていた。\n","（Python→JVM間でデータ移動が発生するため）\n","\n","そこで、Apache Spark 2.3では 「Pandas UDFs」（a.k.a Vectrized UDF）が紹介されている。\n","Pandas UDFではデータ移動にApache Arrowを利用しており、Pandasはそのデータで動作する。\n","pandas_udfデコレータを利用するか、functionをラップしてやることで、データはApache Arrowsフォーマットになり、seliarize/pickleといった、pythonプロセスの負荷の高い処理をしなくて済む。\n","代わりに Pandas Series または DataFrameで処理を組む必要が出てくる。\n","\n","Apache Spark 3.0 + Python 3.5 からは、2つのカテゴリに分けられている。\n","\n","Pandas UDFs:　Pandasの pandas.Series, pandas.DataFrame, Tuple, Iteratorで組まなければならない。\n","https://docs.databricks.com/spark/latest/spark-sql/udf-python-pandas.html\n","\n","Pandas Function APIs:　既存のdefを変換できる。\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Pandas UDF\n","# Import pandas\n","import pandas as pd\n","\n","# Import various pyspark SQL functions including pandas_udf\n","from pyspark.sql.functions import col, pandas_udf\n","from pyspark.sql.types import LongType\n","\n","# Declare the cubed function\n","def cubed(a: pd.Series) -> pd.Series:\n","    return a * a * a\n","\n","# Create the pandas UDF for the cubed function\n","cubed_udf = pandas_udf(cubed, returnType=LongType())\n","\n","# Pandas Sample\n","# Create a Pandas Series\n","x = pd.Series([1, 2, 3])\n","\n","# The function for a pandas_udf executed with local Pandas data\n","print(cubed(x))\n","\n","# Dataframe Sample\n","# Create a Spark DataFrame\n","df = spark.range(1, 4)\n","\n","# Execute function as a Spark vectorized UDF\n","df.select(\"id\", cubed_udf(col(\"id\"))).show()\n"]},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","DataFrameへのレコードの追加\n","withColumnを利用する。第一引数に結果として保存するキー名、第二引数に処理を指定する。\n","\"\"\"\n","df_add_result = df_base\\\n","    .withColumn(\"add_tdate\", F.col(\"tdate\"))"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","DataFrameの定数列追加\n","一律の絡むを追加するには、F.litを利用する\n","\"\"\"\n","df_convert_result = df_base\n","    .withColumn(\"today\", F.lit(\"today\"))"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","DataFrameのカラム型変換\n","\"\"\"\n","# Date\n","df_convert_result1 = df_base\n","    .withColumn(\"tdate\", F.lit(str_yyyymmdd_to_date(target_date)))\\\n","    .withColumn(\"tdate\", F.lit(F.col(\"tdate\").cast(\"date\")))\n","    \n","# Timestamp\n","df_wakati_result2 = df_base\\\n","    .withColumn(\"created_at\", df_wakati_base.created_at.cast(T.TimestampType()))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Array functions\n","https://sparkbyexamples.com/spark/spark-sql-array-functions/\n","\n","Map functions\n","\"\"\""]},{"execution_count":null,"cell_type":"code","source":["# -------------- BigQuery Connector -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","dataproc-mecab-init-shellで作成した環境では、BigQueryコネクタをinstallしているため、\n","BigQueryの読み込み、書き込みができる。\n","https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example?hl=ja\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# GCPのサンプル\n","# Load data from BigQuery.\n","words = spark.read.format('bigquery') \\\n","  .option('table', 'bigquery-public-data:samples.shakespeare') \\\n","  .load()\n","words.createOrReplaceTempView('words')\n","\n","# Perform word count.\n","word_count = spark.sql(\n","    'SELECT word, SUM(word_count) AS word_count FROM words GROUP BY word')\n","word_count.show()\n","word_count.printSchema()\n","\n","# Saving the data to BigQuery\n","word_count.write.format('bigquery').option('table', 'wordcount_dataset.wordcount_output').save()"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","既存のTableへの追加\n","（上記のサンプルはテーブルの新規作成にあたり、既存への追加ではエラーになる。）\n","\"\"\"\n","# df_base\\\n","#     .write\\\n","#     .format('bigquery')\\\n","#     .mode('append')\\ #モードをappendに切り替える\n","#     .option('table', '{0}.{1}'.format(bigquery_dataset, bigquery_save_table))\\\n","#     .option('partitionType', 'DAY')\\  #パーティションタイプを設定する\n","#     .option('partitionField', 'tdate')\\  #パーティションに利用するフィールドを指定する\n","#     .save()"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# -------------- MySQL Connector -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","Dataprocで、MySQLへつなぐ方法としてdataproc-mecab-init-shellで作成した環境では\n","Apache Hiveを応用し、\n","　・enable-cloud-sql-hive-metastore=false （本来のメタストアは有効にしない）\n"," ・additional-cloud-sql-instances=${CLOUDSQL_PROJECT_ID}:${REGION}:${CLOUDSQL_INSTANCE_NAME}=tcp:5432 （追加で接続したいDBを指定）\n","することで、localhost:5432 で接続できるようにしている。\n","\n","また、クライアントとしては JDBC（Java Database Connectivity） をインストールしている。\n","SparkでJDBCを利用する際には、$SPARK_HOME クラスパスにJDBCドライバーを配置しなければならない\n","\n","MySQLの場合：\n"," bin/spark-shell --jars mysql-connector-java_8.0.16-bin.jar\n","\n","これで、他のファイルからの読み込みと同じように操作できる\n","\n","サンプルなどは以下を参照\n","https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["options = {\n","    \"url\":\"jdbc:mysql://127.0.01:5432/{スキーマ名}\",\n","    \"driver\":\"com.mysql.jdbc.Driver\",\n","    \"dbtable\":\"{テーブル名}\",\n","    \"user\":\"{ユーザー名}\",\n","    \"password\":\"{パスワード}\"\n","}\n","\n","df = spark.read.format(\"jdbc\").options(**options).load()\n","df.limit(5).show()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Spark SQLとJDBCの外部ソース間で大量のデータを転送する場合、データソースをパーティショニングすることが重要。\n","\n","すべてのデータが1つのドライバ接続を介して転送されるため、ソースシステムのリソースが飽和してしまい、抽出のパフォーマンスが大幅に低下する可能性がある。\n","\n","JDBCプロパティの以下オプションにて、大規模な操作ではプロパティを使用することを強くオススメされている。\n","「partitionColumnの値を利用し、lowerBoundとupperBoundの間をnumPartitionsの数で割ってパーティションが作られ、それごとに処理される」\n","\n","numPartitions: \n","テーブルの読み書きで並列に使用できるパーティションの最大数。また、これは同時に実行されるJDBC接続の最大数を決定する。\n","良いスタートポイントは、Sparkワーカーの数の倍数を使うこと。例えば、4つのSparkワーカーノードを持っている場合は、4つまたは8つのパーティションから始めるとよい。ただしソースシステムがどれだけ読み込み要求を処理できるかにも注意することが重要。処理ウィンドウがないOLT的なシステムでは、飽和を防ぐために同時リクエスト数を減らすべき。\n","\n","partitionColumn: \n","外部ソースを読み込む場合、partitionColumnはパーティションを決定するために使用されるカラムを指定する。\n","データのskewを避けるために、一様に分散できる partitionColumn を選択すべき。可能であれば新しい partitionColumn (複数のカラムのハッシュ) を生成して、パーティションをより均等に分散させられるとベスト。\n","\n","lowerBound/upperBound: \n","partitionColumnの最小値/最大値をパーティションストライドに設定する。\n","最初は、PartitionColumnの実測値の最小値と最大値に基づいて、lowerBoundと upperBoundを計算する。（実測と離れてた値をlower/upperに設定すると、実際に使われるパーティションが実は2個とかあり得るため）\n","\"\"\""]},{"execution_count":null,"cell_type":"code","source":["# -------------- MeCabを使った形態素解析 -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","dataproc-mecab-init-shellで作成した環境では、MeCabのインストールも行なっている。（拡張辞書入り）\n","MeCabでTokenizerを作り、RDDのmapで実行することで処理が可能。\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["import MeCab\n","class JapaneseTokenizer(object):\n","    def __init__(self):\n","        self.mecab = MeCab.Tagger(\"-Ochasen -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n","        self.mecab.parseToNode('')\n"," \n","    def split(self, text):\n","        node = self.mecab.parseToNode(text)\n","        words = []\n","        while node:\n","            if node.surface:\n","                words.append(node.surface.decode(\"UTF-8\"))\n","            node = node.next\n","        return words\n","\n","def tokenize(text):\n","    tokenizer = JapaneseTokenizer()\n","    return tokenizer.split(text)"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","tokenizeのテスト実行\n","\"\"\"\n","print(tokenize(u'テスト文字列'.encode('utf-8')))"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","本実行（テストとして5件にフィルタしている） RDD版\n","\"\"\"\n","df = spark.read.csv(tsv_path, sep=r'\\t', header=True)\n","results = df.limit(5).select(\"text\").rdd.map(lambda x: ','.join(tokenize(x.text.encode('utf-8'))))"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","本実行 DataFrame版\n","\"\"\"\n","def tokenize_and_create_rdd(text):\n","    return ','.join(tokenize(text.encode(\"UTF-8\")))\n","\n","tokenize_udf = F.udf(tokenize_and_create_rdd, T.StringType())"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","結果チェック\n","\"\"\"\n","for i, result in enumerate(results.take(5)):\n","    print(result)"],"outputs":[],"metadata":{}}],"nbformat":4,"metadata":{"kernelspec":{"display_name":"PySpark","name":"pyspark","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","version":"2.7.14","name":"python","file_extension":".py","pygments_lexer":"ipython2","codemirror_mode":{"version":2,"name":"ipython"}}}}
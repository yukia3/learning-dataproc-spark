{"nbformat_minor": 2, "cells": [{"execution_count": null, "cell_type": "code", "source": "# -------------- Apache Spark\u306b\u3064\u3044\u3066 -------------- ", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nApache Spark\u306f\u3001\u7c21\u5358\u306b\u8a00\u3046\u3068 \u4e26\u5217\u5206\u6563\u51e6\u7406\u57fa\u76e4\u3067\u3042\u308b Hadoop \u306e MapReduce \u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u306b\u76f8\u5f53\u3059\u308b\u3082\u306e\u3002\n\nMapReduce\u51e6\u7406\u306f Map\u51e6\u7406\u2192Reduce\u51e6\u7406 \u3054\u3068\u306b\u57fa\u672c\u7684\u306b\u306f HDFS \u306b\u66f8\u304d\u8fbc\u3093\u3067\u3044\u308b\u3002\n\u305d\u306e\u5834\u5408\u3001\n\u3000\u30fbMap\u51e6\u7406\u2192Reduce\u51e6\u7406\u304c\u591a\u6bb5\u306b\u306a\u3063\u305f\u5834\u5408\u306b\u3001I/O\u306e\u30ec\u30a4\u30c6\u30f3\u30b7\u304c\u554f\u984c\u306b\u306a\u308b\n\u3000\u30fb\u6a5f\u68b0\u5b66\u7fd2\u306e\u3088\u3046\u306a\u3001\u540c\u3058\u30c7\u30fc\u30bf\uff08\u51e6\u7406\u7d50\u679c\uff09\u3092\u4f55\u5ea6\u3082\u4f7f\u3044\u56de\u3059\u51e6\u7406\u306e\u5834\u5408\u3001\u3084\u306f\u308aI/O\u304c\u4f55\u5ea6\u3082\u767a\u751f\u3057\u554f\u984c\u306b\u306a\u308b\n\u3068\u3044\u3063\u305f\u3088\u3046\u306a\u554f\u984c\u304c\u767a\u751f\u3059\u308b\u3002\n\n\u305d\u3053\u3067 Apache Spark \u3067\u306f \u201c\u30a4\u30f3\u30e1\u30e2\u30ea\u201d\u30fb\u201cRDD\u201d\uff08\u90e8\u5206\u6545\u969c\u3078\u306e\u8010\u6027\u3092\u8003\u616e\u3057\u305f\u5206\u6563\u30b3\u30ec\u30af\u30b7\u30e7\u30f3\uff09 \u3092\u6d3b\u7528\u3059\u308b\u3053\u3068\u3067\n\u3053\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3057\u3066\u3044\u304f\u3002\n\n\u3053\u306eRDD\u306f Apache Spark Core \u3068\u3057\u3066\u4f4e\u30ec\u30a4\u30e4\u3092\u62c5\u3063\u3066\u304a\u308a\u3001\nApache Spark Core \u3092\u30d9\u30fc\u30b9\u3068\u3057\u3066\n\u3000\u30fbSpark SQL\n \u3000\uff08\u69cb\u9020\u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u51e6\u7406\u306e\u305f\u3081\u306eSpark\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3001\u69cb\u9020\u5316\u3055\u308c\u3066\u3044\u308c\u3070\u5f62\u5f0f\u306b\u95a2\u308f\u3089\u305a\u3001DataFrame\u3068\u3057\u3066\u8aad\u307f\u8fbc\u3080\u3053\u3068\u3067SQL\u7684\u306b\u30c7\u30fc\u30bf\u3092\u6271\u3048\u308b\uff09\n \u30fbSpark Streaming\n \u3000\uff08\u30b9\u30c8\u30ea\u30fc\u30e0\u30c7\u30fc\u30bf\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306eSpark\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\uff09\n \u30fbMLib\n \u3000\uff08\u6a5f\u68b0\u5b66\u7fd2\u306e\u305f\u3081\u306eSpark\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3001\u6700\u65b0\u3067\u306fSpark SQL\u306b\u5165\u3063\u3066\u3044\u308b\uff1f\uff09\n \u30fbGraphX\n \u3000\uff08\u5927\u5bb9\u91cf\u306e\u30b0\u30e9\u30d5\u30c7\u30fc\u30bf\u3092\u4e26\u5217\u5206\u6563\u74b0\u5883\u3067\u51e6\u7406\u3059\u308b\u305f\u3081\u306e\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u3001\u30ca\u30ec\u30c3\u30b8\u30b0\u30e9\u30d5\u3001\u30de\u30fc\u30b1\u30c6\u30a3\u30f3\u30b0\u30ea\u30ec\u30fc\u30b7\u30e7\u30f3\u3001\u7d4c\u8def\u63a2\u7d22\u3001\u30da\u30fc\u30b8\u30e9\u30f3\u30af\u5206\u6790\u306a\u3069\u7279\u5b9a\u306e\u5206\u91ce\u306b\u5bfe\u3057\u3066\u306f\u975e\u5e38\u306b\u6709\u52b9\uff09\n\u3068\u3044\u3063\u305f\u30b3\u30f3\u30dd\u30fc\u30cd\u30f3\u30c8\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u3002\n\"\"\"", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# -------------- \u524d\u63d0\u3068\u3057\u3066\u3084\u308b\u3079\u304d\u3053\u3068 -------------- ", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nSparkSession\u3092import\u3059\u308b\n\"\"\"\nfrom pyspark.sql import SparkSession", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nSparkSession\u3067\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u4f5c\u6210\u3059\u308b\u3002\n\u306a\u304a\u3001\u3059\u3067\u306b\u540c\u4e00\u540d\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u65e2\u5b58\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u53d6\u5f97\u3059\u308b\u3002\n\"\"\"\nspark = SparkSession \\\n  .builder \\\n  .master('yarn') \\\n  .appName('learning-spark') \\\n  .getOrCreate()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nDataproc\u3067\u306f\u30c7\u30fc\u30bf\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u306e\u4e00\u6642\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068\u3057\u3066\u3001GCS\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\u7279\u306bBigQuery\u30b3\u30cd\u30af\u30bf\u3092\u5229\u7528\u3057\u3066\u53d6\u5f97\u3059\u308b\u969b\u306b\u306f\u91cd\u8981\u306b\u306a\u308b\n\"\"\"\nbucket = \"\u301c\"\nspark.conf.set('temporaryGcsBucket', bucket)", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# -------------- Spark SQL -------------- ", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# -------------- DataFrame \u3068 RDD -------------- ", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nDataFrame\u3068\u306f\u3001Spark\u4e0a\u3067\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u30c6\u30fc\u30d6\u30eb\u306e\u3088\u3046\u306b\u3057\u3066\u6271\u3046\u305f\u3081\u306e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3002\n\n\u3053\u308c\u307e\u3067Spark SQL\u3067\u306fRDD\u304c\u30e1\u30a4\u30f3\u3067\u4f7f\u308f\u308c\u3066\u3044\u305f\u304c\u3001\u4f4d\u7f6e\u4ed8\u3051\u3068\u3057\u3066\u306fDataFrame\u306f\u3088\u308a\u9ad8\u30ec\u30a4\u30e4\u306a\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067\u3042\u308a\u3001\nfilter\u3084join\u306a\u3069\u306e\u4fbf\u5229\u306a\u30e1\u30bd\u30c3\u30c9\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u3002RDD\u3078\u306e\u5909\u63db\u30fbRDD\u304b\u3089\u306e\u5909\u63db\u3082\u53ef\u80fd\u3002\n\n\u7528\u610f\u3055\u308c\u3066\u3044\u308bAPI\u3067\u3067\u304d\u308b\u51e6\u7406\u306f\u3001DataFrame\u306e\u65b9\u304c\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3082\u51fa\u305b\u308b\u305f\u3081\u3001\u4eca\u5f8c\u306fDataFrame\u3067\u51e6\u7406\u3092\u7d44\u307f\u3064\u3064\u3001\n\u51e6\u7406\u4e0a\u5fc5\u8981\u306b\u306a\u3063\u305f\u969b\u306b\u306fRDD\u306b\u5909\u63db\u3057\u3066\u5bfe\u5fdc\u3057\u3066\u3044\u304f\u3001\u3068\u3044\u3046\u3053\u3068\u306b\u306a\u308b\u3002\n\nDataFrame\u306f\u3001spark\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u4f7f\u3063\u3066\n\u3000\u30fb\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092read\u3059\u308b\n \u30fb\u4ed6\u306eDataFrame\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\n\u306a\u3069\u3057\u305f\u5834\u5408\u306b\u751f\u6210\u3067\u304d\u308b\u3002\uff08SparkSession\u304c\u5b89\u5168\u306b\u3001\u5206\u6563\u51e6\u7406\u3092\u5b9f\u73fe\u3057\u3066\u304f\u308c\u308b\uff09\n\"\"\"", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nDataproc\u3067\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067GCS\u30b3\u30cd\u30af\u30bf\u304c\u5165\u3063\u3066\u304a\u308a\u3001\npath\u306b \u201cgs://\u301c\u201d \u3068\u5bfe\u8c61\u30d5\u30a1\u30a4\u30eb\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067\u3001\u81ea\u7136\u306b\u53d6\u5f97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\n\"\"\"\ntsv_path=\"gs://\u301c.tsv\"\ndf = spark.read.csv(tsv_path, sep=r'\\t', header=True)", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nlimit \u3084 select\u3067\u3001SQL\u30e9\u30a4\u30af\u306b\u30c7\u30fc\u30bf\u3092\u6271\u3048\u308b\n\"\"\"\ndf.limit(5).select(\"text\").show()\nprint(df.count())\nprint(df.columns)", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nRDD\u3078\u306e\u5909\u63db\u306f df.rdd \u3067\u884c\u3048\u308b\u3002\n\u305f\u3060\u3057 RDD[Row] \u5f62\u5f0f\u3067\u8fd4\u3063\u3066\u304f\u308b\u306e\u3067\u3001map\u5185\u3067\u306f\u3055\u3089\u306b\u30d5\u30a3\u30fc\u30eb\u30c9\u3092\u6307\u5b9a\u3057\u3066\u3084\u308b\u5fc5\u8981\u3042\u308a \uff08select\u3067\u7279\u5b9a\u30d5\u30a3\u30fc\u30eb\u30c9\u306b\u7d5e\u3063\u3066\u3082\u30c0\u30e1\uff09\nhttps://stackoverflow.com/questions/40653567/attribute-error-split-on-spark-sql-python-using-lambda\n\"\"\"\ndf.limit(4).select('idx_id').rdd.map(lambda line: line.idx_id.split(\"_\")).collect()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nRDD\u304b\u3089DataFrame\u306e\u5909\u63db\u3082\u884c\u3048\u308b\nhttps://blog.imind.jp/entry/2019/06/23/004922\n\"\"\"\nfrom pyspark.sql import types as T, functions as F\n\n# \u30b9\u30ad\u30fc\u30de\u306e\u8a2d\u5b9a\nschema = T.StructType([\n    T.StructField('col1', T.StringType()),\n    T.StructField('col2', T.LongType())\n])\n\n# \u5148\u307b\u3069\u30a8\u30e9\u30fc\u306b\u306a\u3063\u305frdd\nrdd2 = sc.parallelize([\n    Row(col1=None, col2=1),\n    Row(col1=None, col2=2)\n])\n\n# schema\u3092\u6307\u5b9a\u3057\u3066DataFrame\u306b\u5909\u63db\ndf2 = spark.createDataFrame(rdd2, schema)\ndf2.collect()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nSpark SQL\u3092jupyter\u3067\u89e6\u3063\u3066\u3044\u3066\u3001select\u306a\u3069\u3092\u4f7f\u3063\u3066\u3044\u308b\u3068\u300c\u3042\u308c\uff1f\u3053\u3093\u306a\u306b\u65e9\u304fselect\u3067\u304d\u308b\u306e\uff1f\u300d\u3068\u601d\u3046\u5834\u9762\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n\nlimit\u3084select\u306a\u3069\u306efunc\u3092\u547c\u3093\u3067\u3044\u308b\u969b\u306b\u306f\u51e6\u7406\u306e\u6d41\u308c\u3092\u7d44\u3093\u3067\u3044\u308b\u72b6\u614b\n\uff08\u3053\u308c\u3089\u306e\u51e6\u7406\u306f\u300cTransformations\u300d\u3068\u547c\u3070\u308c\u308b\u5834\u5408\u3082\u3002\n\u3000RDD\u3067\u8a00\u3046\u3068\u3001https://ex-ture.com/blog/2019/06/27/learn-databricks-spark-rdd-operations/\n    map\n    flatMap\n    filter\n    union\n    intersection\n    subtrct\n    distinct\n\u3000\u3000\u306a\u3069\u304c\u3042\u308a\u3001DataFrame\u3067\u8a00\u3046\u3068 https://www.learningjournal.guru/courses/spark/spark-foundation-training/spark-dataframe-transformations/\n     select\n    groupby\n\u3000\u306a\u3069\u304c\u3042\u308b\uff09\n\n\u305d\u308c\u304c\u5b9f\u969b\u306b\u51e6\u7406\u3055\u308c\u308b\u306e\u306f\u300cActions\u300d\u3068\u3044\u3046\u201c\u7d50\u679c\u3092\u53d6\u5f97\u3059\u308bfunc\u201d\u304c\u547c\u3070\u308c\u305f\u3068\u304d\u306b\u521d\u3081\u3066\u5b9f\u884c\u3055\u308c\u308b\u3002\nRDD\u3067\u8a00\u3046\u3068\u3001https://ex-ture.com/blog/2019/06/27/learn-databricks-spark-rdd-operations/\n    collect\n    count\n    first\n    take\n    reduce\n    takeOrdered\n    top\n\u306a\u3069\u304c\u3042\u308a\u3001DataFrame\u3067\u8a00\u3046\u3068 \n    show\n\u3084\u3001\u5404\u7a2e\u51fa\u529b\u304c\u6319\u3052\u3089\u308c\u308b\u3002\n\nActions\u306e\u591a\u304f\u306f\u3001\u7d50\u679c\u3068\u3057\u3066Python\u306e\u914d\u5217\u3092\u8fd4\u3059\u3053\u3068\u3082\u591a\u3044\u3002\n\uff08\u9006\u3092\u8a00\u3046\u3068\u3001RDD\u306fSpark\u306e\u578b\u306a\u306e\u3067\u3001\u901a\u5e38\u306ePython\u306efunc\u51e6\u7406\u3092\u4f7f\u304a\u3046\u3068\u3059\u308b\u969b\u306b\u306f\u6ce8\u610f\u304c\u5fc5\u8981\uff09\n\"\"\"", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nDataFrame\u306eUDF\nRDD\u3067\u306fmap\u3084filter\u3067\u5404\u30ec\u30b3\u30fc\u30c9\u3054\u3068\u306b\u51e6\u7406\u3092\u3057\u3066\u3044\u304f\u304c\u3001\nDataFrame\u3067\u306fUDF\uff08\u30e6\u30fc\u30b6\u30fc\u5b9a\u7fa9\u95a2\u6570\uff09\u3092\u4f7f\u3063\u3066\u51e6\u7406\u3092\u3057\u3066\u3044\u304f\u3002\nhttps://spark.apache.org/docs/2.4.0/api/python/_modules/pyspark/sql/udf.html\n\"\"\"\nclass JapaneseTokenizer(object):\n    def __init__(self):\n        self.mecab = MeCab.Tagger(\"-Ochasen -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n        self.mecab.parseToNode('')\n \n    def split(self, text):\n        node = self.mecab.parseToNode(text)\n        words = []\n        while node:\n            if node.surface:\n                words.append(node.surface.decode(\"UTF-8\"))\n            node = node.next\n        return words\ndef tokenize(text):\n    tokenizer = JapaneseTokenizer()\n    return tokenizer.split(text)\ndef tokenize_and_create_rdd(text):\n    return ','.join(tokenize(text.encode(\"UTF-8\")))\n\ntokenize_udf = F.udf(tokenize_and_create_rdd, T.StringType()) #\u65e2\u5b58\u306edef\u3092\u7b2c\u4e00\u5f15\u6570\u306b\u3001\u7b2c\u4e8c\u5f15\u6570\u306b\u623b\u308a\u5024\u306eSpark\u578b\u3092\u5165\u308c\u3066\u6559\u3048\u3066\u3042\u3052\u308b\n\ndf_wakati_result = df_wakati_base\\\n    .withColumn(\"wakati\", tokenize_udf((F.col(\"text\"))))", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nDataFrame\u3078\u306e\u30ec\u30b3\u30fc\u30c9\u306e\u8ffd\u52a0\nwithColumn\u3092\u5229\u7528\u3059\u308b\u3002\u7b2c\u4e00\u5f15\u6570\u306b\u7d50\u679c\u3068\u3057\u3066\u4fdd\u5b58\u3059\u308b\u30ad\u30fc\u540d\u3001\u7b2c\u4e8c\u5f15\u6570\u306b\u51e6\u7406\u3092\u6307\u5b9a\u3059\u308b\u3002\n\"\"\"\ndf_add_result = df_base\\\n    .withColumn(\"add_tdate\", F.col(\"tdate\"))", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nDataFrame\u306e\u5b9a\u6570\u5217\u8ffd\u52a0\n\u4e00\u5f8b\u306e\u7d61\u3080\u3092\u8ffd\u52a0\u3059\u308b\u306b\u306f\u3001F.lit\u3092\u5229\u7528\u3059\u308b\n\"\"\"\ndf_convert_result = df_base\n    .withColumn(\"today\", F.lit(\"today\"))", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nDataFrame\u306e\u30ab\u30e9\u30e0\u578b\u5909\u63db\n\"\"\"\n# Date\ndf_convert_result1 = df_base\n    .withColumn(\"tdate\", F.lit(str_yyyymmdd_to_date(target_date)))\\\n    .withColumn(\"tdate\", F.lit(F.col(\"tdate\").cast(\"date\")))\n    \n# Timestamp\ndf_wakati_result2 = df_base\\\n    .withColumn(\"created_at\", df_wakati_base.created_at.cast(T.TimestampType()))", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# -------------- BigQuery Connector -------------- ", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\ndataproc-mecab-init-shell\u3067\u4f5c\u6210\u3057\u305f\u74b0\u5883\u3067\u306f\u3001BigQuery\u30b3\u30cd\u30af\u30bf\u3092install\u3057\u3066\u3044\u308b\u305f\u3081\u3001\nBigQuery\u306e\u8aad\u307f\u8fbc\u307f\u3001\u66f8\u304d\u8fbc\u307f\u304c\u3067\u304d\u308b\u3002\nhttps://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example?hl=ja\n\"\"\"", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# GCP\u306e\u30b5\u30f3\u30d7\u30eb\n# Load data from BigQuery.\nwords = spark.read.format('bigquery') \\\n  .option('table', 'bigquery-public-data:samples.shakespeare') \\\n  .load()\nwords.createOrReplaceTempView('words')\n\n# Perform word count.\nword_count = spark.sql(\n    'SELECT word, SUM(word_count) AS word_count FROM words GROUP BY word')\nword_count.show()\nword_count.printSchema()\n\n# Saving the data to BigQuery\nword_count.write.format('bigquery').option('table', 'wordcount_dataset.wordcount_output').save()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n\u65e2\u5b58\u306eTable\u3078\u306e\u8ffd\u52a0\n\uff08\u4e0a\u8a18\u306e\u30b5\u30f3\u30d7\u30eb\u306f\u30c6\u30fc\u30d6\u30eb\u306e\u65b0\u898f\u4f5c\u6210\u306b\u3042\u305f\u308a\u3001\u65e2\u5b58\u3078\u306e\u8ffd\u52a0\u3067\u306f\u30a8\u30e9\u30fc\u306b\u306a\u308b\u3002\uff09\n\"\"\"\n# df_base\\\n#     .write\\\n#     .format('bigquery')\\\n#     .mode('append')\\ #\u30e2\u30fc\u30c9\u3092append\u306b\u5207\u308a\u66ff\u3048\u308b\n#     .option('table', '{0}.{1}'.format(bigquery_dataset, bigquery_save_table))\\\n#     .option('partitionType', 'DAY')\\  #\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u30bf\u30a4\u30d7\u3092\u8a2d\u5b9a\u3059\u308b\n#     .option('partitionField', 'tdate')\\  #\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u306b\u5229\u7528\u3059\u308b\u30d5\u30a3\u30fc\u30eb\u30c9\u3092\u6307\u5b9a\u3059\u308b\n#     .save()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# -------------- MySQL Connector -------------- ", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\nDataproc\u3067\u3001MySQL\u3078\u3064\u306a\u3050\u65b9\u6cd5\u3068\u3057\u3066dataproc-mecab-init-shell\u3067\u4f5c\u6210\u3057\u305f\u74b0\u5883\u3067\u306f\nApache Hive\u3092\u5fdc\u7528\u3057\u3001\n\u3000\u30fbenable-cloud-sql-hive-metastore=false \uff08\u672c\u6765\u306e\u30e1\u30bf\u30b9\u30c8\u30a2\u306f\u6709\u52b9\u306b\u3057\u306a\u3044\uff09\n \u30fbadditional-cloud-sql-instances=${CLOUDSQL_PROJECT_ID}:${REGION}:${CLOUDSQL_INSTANCE_NAME}=tcp:5432 \uff08\u8ffd\u52a0\u3067\u63a5\u7d9a\u3057\u305f\u3044DB\u3092\u6307\u5b9a\uff09\n\u3059\u308b\u3053\u3068\u3067\u3001localhost:5432 \u3067\u63a5\u7d9a\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u308b\u3002\n\n\u307e\u305f\u3001\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3068\u3057\u3066\u306f JDBC\uff08Java Database Connectivity\uff09 \u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u3044\u308b\u3002\n\u3053\u308c\u3067\u3001\u4ed6\u306e\u30d5\u30a1\u30a4\u30eb\u304b\u3089\u306e\u8aad\u307f\u8fbc\u307f\u3068\u540c\u3058\u3088\u3046\u306b\u64cd\u4f5c\u3067\u304d\u308b\n\"\"\"", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "options = {\n    \"url\":\"jdbc:mysql://127.0.01:5432/{\u30b9\u30ad\u30fc\u30de\u540d}\",\n    \"driver\":\"com.mysql.jdbc.Driver\",\n    \"dbtable\":\"{\u30c6\u30fc\u30d6\u30eb\u540d}\",\n    \"user\":\"{\u30e6\u30fc\u30b6\u30fc\u540d}\",\n    \"password\":\"{\u30d1\u30b9\u30ef\u30fc\u30c9}\"\n}\n\ndf = spark.read.format(\"jdbc\").options(**options).load()\ndf.limit(5).show()", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# -------------- MeCab\u3092\u4f7f\u3063\u305f\u5f62\u614b\u7d20\u89e3\u6790 -------------- ", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\ndataproc-mecab-init-shell\u3067\u4f5c\u6210\u3057\u305f\u74b0\u5883\u3067\u306f\u3001MeCab\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3082\u884c\u306a\u3063\u3066\u3044\u308b\u3002\uff08\u62e1\u5f35\u8f9e\u66f8\u5165\u308a\uff09\nMeCab\u3067Tokenizer\u3092\u4f5c\u308a\u3001RDD\u306emap\u3067\u5b9f\u884c\u3059\u308b\u3053\u3068\u3067\u51e6\u7406\u304c\u53ef\u80fd\u3002\n\"\"\"", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import MeCab\nclass JapaneseTokenizer(object):\n    def __init__(self):\n        self.mecab = MeCab.Tagger(\"-Ochasen -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n        self.mecab.parseToNode('')\n \n    def split(self, text):\n        node = self.mecab.parseToNode(text)\n        words = []\n        while node:\n            if node.surface:\n                words.append(node.surface.decode(\"UTF-8\"))\n            node = node.next\n        return words\n\ndef tokenize(text):\n    tokenizer = JapaneseTokenizer()\n    return tokenizer.split(text)", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\ntokenize\u306e\u30c6\u30b9\u30c8\u5b9f\u884c\n\"\"\"\nprint(tokenize(u'\u30c6\u30b9\u30c8\u6587\u5b57\u5217'.encode('utf-8')))", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n\u672c\u5b9f\u884c\uff08\u30c6\u30b9\u30c8\u3068\u3057\u30665\u4ef6\u306b\u30d5\u30a3\u30eb\u30bf\u3057\u3066\u3044\u308b\uff09 RDD\u7248\n\"\"\"\ndf = spark.read.csv(tsv_path, sep=r'\\t', header=True)\nresults = df.limit(5).select(\"text\").rdd.map(lambda x: ','.join(tokenize(x.text.encode('utf-8'))))", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n\u672c\u5b9f\u884c DataFrame\u7248\n\"\"\"\ndef tokenize_and_create_rdd(text):\n    return ','.join(tokenize(text.encode(\"UTF-8\")))\n\ntokenize_udf = F.udf(tokenize_and_create_rdd, T.StringType())", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\"\"\"\n\u7d50\u679c\u30c1\u30a7\u30c3\u30af\n\"\"\"\nfor i, result in enumerate(results.take(5)):\n    print(result)", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}
{"nbformat_minor":2,"cells":[{"execution_count":null,"cell_type":"code","source":["# -------------- Apache Sparkについて -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","Apache Sparkは、簡単に言うと 並列分散処理基盤である Hadoop の MapReduce フレームワークに相当するもの。\n","\n","MapReduce処理は Map処理→Reduce処理 ごとに基本的には HDFS に書き込んでいる。\n","その場合、\n","　・Map処理→Reduce処理が多段になった場合に、I/Oのレイテンシが問題になる\n","　・機械学習のような、同じデータ（処理結果）を何度も使い回す処理の場合、やはりI/Oが何度も発生し問題になる\n","といったような問題が発生する。\n","\n","そこで Apache Spark では “インメモリ”・“RDD”（部分故障への耐性を考慮した分散コレクション） を活用することで\n","この問題を解決していく。\n","\n","このRDDは Apache Spark Core として低レイヤを担っており、\n","Apache Spark Core をベースとして\n","　・Spark SQL\n"," 　（構造化されたデータの処理のためのSparkコンポーネント、構造化されていれば形式に関わらず、DataFrameとして読み込むことでSQL的にデータを扱える）\n"," ・Spark Streaming\n"," 　（ストリームデータを処理するためのSparkコンポーネント）\n"," ・MLib\n"," 　（機械学習のためのSparkコンポーネント、最新ではSpark SQLに入っている？）\n"," ・GraphX\n"," 　（大容量のグラフデータを並列分散環境で処理するためのコンポーネント、ナレッジグラフ、マーケティングリレーション、経路探索、ページランク分析など特定の分野に対しては非常に有効）\n","といったコンポーネントが用意されている。\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# -------------- 前提としてやるべきこと -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","SparkSessionをimportする\n","\"\"\"\n","from pyspark.sql import SparkSession"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","SparkSessionでセッションを作成する。\n","なお、すでに同一名のセッションが存在する場合は既存セッションを取得する。\n","\"\"\"\n","spark = SparkSession \\\n","  .builder \\\n","  .master('yarn') \\\n","  .appName('learning-spark') \\\n","  .getOrCreate()"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","Dataprocではデータエクスポートの一時ディレクトリとして、GCSを利用することができる。\n","特にBigQueryコネクタを利用して取得する際には重要になる\n","\"\"\"\n","bucket = \"〜\"\n","spark.conf.set('temporaryGcsBucket', bucket)"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# -------------- Spark SQL -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# -------------- DataFrame と RDD -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","DataFrameとは、Spark上でデータファイルをデータベーステーブルのようにして扱うためのオブジェクト。\n","\n","これまでSpark SQLではRDDがメインで使われていたが、位置付けとしてはDataFrameはより高レイヤなオブジェクトであり、\n","filterやjoinなどの便利なメソッドが用意されている。RDDへの変換・RDDからの変換も可能。\n","\n","用意されているAPIでできる処理は、DataFrameの方がパフォーマンスも出せるため、今後はDataFrameで処理を組みつつ、\n","処理上必要になった際にはRDDに変換して対応していく、ということになる。\n","\n","DataFrameは、sparkセッションを使って\n","　・データファイルをreadする\n"," ・他のDataFrameをフィルタリングする\n","などした場合に生成できる。（SparkSessionが安全に、分散処理を実現してくれる）\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","DataprocではデフォルトでGCSコネクタが入っており、\n","pathに “gs://〜” と対象ファイルを指定することで、自然に取得することができる。\n","\"\"\"\n","tsv_path=\"gs://〜.tsv\"\n","df = spark.read.csv(tsv_path, sep=r'\\t', header=True)"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","limit や selectで、SQLライクにデータを扱える\n","\"\"\"\n","df.limit(5).select(\"text\").show()\n","print(df.count())\n","print(df.columns)"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","RDDへの変換は df.rdd で行える。\n","ただし RDD[Row] 形式で返ってくるので、map内ではさらにフィールドを指定してやる必要あり （selectで特定フィールドに絞ってもダメ）\n","https://stackoverflow.com/questions/40653567/attribute-error-split-on-spark-sql-python-using-lambda\n","\"\"\"\n","df.limit(4).select('idx_id').rdd.map(lambda line: line.idx_id.split(\"_\")).collect()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","RDDからDataFrameの変換も行える\n","https://blog.imind.jp/entry/2019/06/23/004922\n","\"\"\"\n","from pyspark.sql import types as T, functions as F\n","\n","# スキーマの設定\n","schema = T.StructType([\n","    T.StructField('col1', T.StringType()),\n","    T.StructField('col2', T.LongType())\n","])\n","\n","# 先ほどエラーになったrdd\n","rdd2 = sc.parallelize([\n","    Row(col1=None, col2=1),\n","    Row(col1=None, col2=2)\n","])\n","\n","# schemaを指定してDataFrameに変換\n","df2 = spark.createDataFrame(rdd2, schema)\n","df2.collect()"]},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","Spark SQLをjupyterで触っていて、selectなどを使っていると「あれ？こんなに早くselectできるの？」と思う場面があるかもしれない。\n","\n","limitやselectなどのfuncを呼んでいる際には処理の流れを組んでいる状態\n","（これらの処理は「Transformations」と呼ばれる場合も。\n","　RDDで言うと、https://ex-ture.com/blog/2019/06/27/learn-databricks-spark-rdd-operations/\n","    map\n","    flatMap\n","    filter\n","    union\n","    intersection\n","    subtrct\n","    distinct\n","　　などがあり、DataFrameで言うと https://www.learningjournal.guru/courses/spark/spark-foundation-training/spark-dataframe-transformations/\n","     select\n","    groupby\n","　などがある）\n","\n","それが実際に処理されるのは「Actions」という“結果を取得するfunc”が呼ばれたときに初めて実行される。\n","RDDで言うと、https://ex-ture.com/blog/2019/06/27/learn-databricks-spark-rdd-operations/\n","    collect\n","    count\n","    first\n","    take\n","    reduce\n","    takeOrdered\n","    top\n","などがあり、DataFrameで言うと \n","    show\n","や、各種出力が挙げられる。\n","\n","Actionsの多くは、結果としてPythonの配列を返すことも多い。\n","（逆を言うと、RDDはSparkの型なので、通常のPythonのfunc処理を使おうとする際には注意が必要）\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# -------------- BigQuery Connector -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","dataproc-mecab-init-shellで作成した環境では、BigQueryコネクタをinstallしているため、\n","BigQueryの読み込み、書き込みができる。\n","https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example?hl=ja\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# GCPのサンプル\n","# Load data from BigQuery.\n","words = spark.read.format('bigquery') \\\n","  .option('table', 'bigquery-public-data:samples.shakespeare') \\\n","  .load()\n","words.createOrReplaceTempView('words')\n","\n","# Perform word count.\n","word_count = spark.sql(\n","    'SELECT word, SUM(word_count) AS word_count FROM words GROUP BY word')\n","word_count.show()\n","word_count.printSchema()\n","\n","# Saving the data to BigQuery\n","word_count.write.format('bigquery').option('table', 'wordcount_dataset.wordcount_output').save()\n","\n","\"\"\"\n","この例ではテーブルごと新規作成となっているが、追加をする場合には\n","word_count.write.format('bigquery').option('table', 'wordcount_dataset.wordcount_output').mode(\"append\").save()\n","というように、モード指定をしてあげる必要がある。\n","\n","また、既存テーブルへの保存はスキーマが一致していないとエラーになるので注意が必要。\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# -------------- MySQL Connector -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","Dataprocで、MySQLへつなぐ方法としてdataproc-mecab-init-shellで作成した環境では\n","Apache Hiveを応用し、\n","　・enable-cloud-sql-hive-metastore=false （本来のメタストアは有効にしない）\n"," ・additional-cloud-sql-instances=${CLOUDSQL_PROJECT_ID}:${REGION}:${CLOUDSQL_INSTANCE_NAME}=tcp:5432 （追加で接続したいDBを指定）\n","することで、localhost:5432 で接続できるようにしている。\n","\n","また、クライアントとしては JDBC（Java Database Connectivity） をインストールしている。\n","これで、他のファイルからの読み込みと同じように操作できる\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["options = {\n","    \"url\":\"jdbc:mysql://127.0.01:5432/{スキーマ名}\",\n","    \"driver\":\"com.mysql.jdbc.Driver\",\n","    \"dbtable\":\"{テーブル名}\",\n","    \"user\":\"{ユーザー名}\",\n","    \"password\":\"{パスワード}\"\n","}\n","\n","df = spark.read.format(\"jdbc\").options(**options).load()\n","df.limit(5).show()"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["# -------------- MeCabを使った形態素解析 -------------- "],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","dataproc-mecab-init-shellで作成した環境では、MeCabのインストールも行なっている。（拡張辞書入り）\n","MeCabでTokenizerを作り、RDDのmapで実行することで処理が可能。\n","\"\"\""],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["import MeCab\n","class JapaneseTokenizer(object):\n","    def __init__(self):\n","        self.mecab = MeCab.Tagger(\"-Ochasen -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\")\n","        self.mecab.parseToNode('')\n"," \n","    def split(self, text):\n","        node = self.mecab.parseToNode(text)\n","        words = []\n","        while node:\n","            if node.surface:\n","                words.append(node.surface)\n","            node = node.next\n","        return words\n","\n","def tokenize(text):\n","    tokenizer = JapaneseTokenizer()\n","    return tokenizer.split(text)"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","tokenizeのテスト実行\n","\"\"\"\n","print(tokenize(u'テスト文字列'.encode('utf-8')))"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","本実行（テストとして5件にフィルタしている）\n","\"\"\"\n","df = spark.read.csv(tsv_path, sep=r'\\t', header=True)\n","results = df.limit(5).select(\"text\").rdd.map(lambda x: ','.join(tokenize(x.text.encode('utf-8'))))"],"outputs":[],"metadata":{}},{"execution_count":null,"cell_type":"code","source":["\"\"\"\n","結果チェック\n","\"\"\"\n","for i, result in enumerate(results.take(5)):\n","    print(result)"],"outputs":[],"metadata":{}}],"nbformat":4,"metadata":{"kernelspec":{"display_name":"PySpark","name":"pyspark","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","version":"2.7.14","name":"python","file_extension":".py","pygments_lexer":"ipython2","codemirror_mode":{"version":2,"name":"ipython"}}}}